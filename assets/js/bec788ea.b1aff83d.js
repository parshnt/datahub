"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[8815],{3905:(e,t,a)=>{a.d(t,{Zo:()=>s,kt:()=>c});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var u=n.createContext({}),p=function(e){var t=n.useContext(u),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},s=function(e){var t=p(e.components);return n.createElement(u.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,u=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),m=p(a),c=r,f=m["".concat(u,".").concat(c)]||m[c]||d[c]||l;return a?n.createElement(f,i(i({ref:t},s),{},{components:a})):n.createElement(f,i({ref:t},s))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=m;var o={};for(var u in t)hasOwnProperty.call(t,u)&&(o[u]=t[u]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var p=2;p<l;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},29313:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var n=a(87462),r=(a(67294),a(3905));const l={title:"Datahub Airflow Plugin",slug:"/metadata-ingestion-modules/airflow-plugin",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/README.md"},i="Datahub Airflow Plugin",o={unversionedId:"metadata-ingestion-modules/airflow-plugin/README",id:"metadata-ingestion-modules/airflow-plugin/README",title:"Datahub Airflow Plugin",description:"Capabilities",source:"@site/genDocs/metadata-ingestion-modules/airflow-plugin/README.md",sourceDirName:"metadata-ingestion-modules/airflow-plugin",slug:"/metadata-ingestion-modules/airflow-plugin",permalink:"/docs/metadata-ingestion-modules/airflow-plugin",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/README.md",tags:[],version:"current",frontMatter:{title:"Datahub Airflow Plugin",slug:"/metadata-ingestion-modules/airflow-plugin",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/README.md"}},u={},p=[{value:"Capabilities",id:"capabilities",level:2},{value:"Installation",id:"installation",level:2},{value:"How to validate installation",id:"how-to-validate-installation",level:2},{value:"Additional references",id:"additional-references",level:2}],s={toc:p};function d(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},s,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"datahub-airflow-plugin"},"Datahub Airflow Plugin"),(0,r.kt)("h2",{id:"capabilities"},"Capabilities"),(0,r.kt)("p",null,"DataHub supports integration of"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Airflow Pipeline (DAG) metadata"),(0,r.kt)("li",{parentName:"ul"},"DAG and Task run information"),(0,r.kt)("li",{parentName:"ul"},"Lineage information when present")),(0,r.kt)("h2",{id:"installation"},"Installation"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"You need to install the required dependency in your airflow."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"  pip install acryl-datahub-airflow-plugin\n")))),(0,r.kt)("p",null,"::: note"),(0,r.kt)("p",null,"We recommend you use the lineage plugin if you are on Airflow version >= 2.0.2 or on MWAA with an Airflow version >= 2.0.2\n:::"),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Disable lazy plugin load in your airflow.cfg"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"core.lazy_load_plugins : False\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"You must configure an Airflow hook for Datahub. We support both a Datahub REST hook and a Kafka-based hook, but you only need one."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"# For REST-based:\nairflow connections add  --conn-type 'datahub_rest' 'datahub_rest_default' --conn-host 'http://localhost:8080'\n# For Kafka-based (standard Kafka sink config can be passed via extras):\nairflow connections add  --conn-type 'datahub_kafka' 'datahub_kafka_default' --conn-host 'broker:9092' --conn-extra '{}'\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Add your ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub_conn_id")," and/or ",(0,r.kt)("inlineCode",{parentName:"p"},"cluster")," to your ",(0,r.kt)("inlineCode",{parentName:"p"},"airflow.cfg")," file if it is not align with the default values. See configuration parameters below"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Configuration options:")),(0,r.kt)("table",{parentName:"li"},(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Default value"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"datahub.datahub_conn_id"),(0,r.kt)("td",{parentName:"tr",align:null},"datahub_rest_deafault"),(0,r.kt)("td",{parentName:"tr",align:null},"The name of the datahub connection you set in step 1.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"datahub.cluster"),(0,r.kt)("td",{parentName:"tr",align:null},"prod"),(0,r.kt)("td",{parentName:"tr",align:null},"name of the airflow cluster")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"datahub.capture_ownership_info"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null},"If true, the owners field of the DAG will be capture as a DataHub corpuser.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"datahub.capture_tags_info"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null},"If true, the tags field of the DAG will be captured as DataHub tags.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"datahub.graceful_exceptions"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null},"If set to true, most runtime errors in the lineage backend will be suppressed and will not cause the overall task to fail. Note that configuration issues will still throw exceptions."))))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Configure ",(0,r.kt)("inlineCode",{parentName:"p"},"inlets")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"outlets")," for your Airflow operators. For reference, look at the sample DAG in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub_provider/example_dags/lineage_backend_demo.py"},(0,r.kt)("inlineCode",{parentName:"a"},"lineage_backend_demo.py")),", or reference ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py"},(0,r.kt)("inlineCode",{parentName:"a"},"lineage_backend_taskflow_demo.py"))," if you're using the ",(0,r.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/concepts/taskflow.html"},"TaskFlow API"),".")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"[optional]"," Learn more about ",(0,r.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/lineage.html"},"Airflow lineage"),", including shorthand notation and some automation."))),(0,r.kt)("h2",{id:"how-to-validate-installation"},"How to validate installation"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Go and check in Airflow at Admin -> Plugins menu if you can see the Datahub plugin")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Run an Airflow DAG and you should see in the task logs Datahub releated log messages like:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"Emitting Datahub ...\n")))),(0,r.kt)("h2",{id:"additional-references"},"Additional references"),(0,r.kt)("p",null,"Related Datahub videos:\n",(0,r.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=3wiaqhb8UR0"},"Airflow Lineage"),"\n",(0,r.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=YpUOqDU5ZYg"},"Airflow Run History in DataHub")))}d.isMDXComponent=!0}}]);